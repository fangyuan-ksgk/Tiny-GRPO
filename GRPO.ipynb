{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec5d0e27-1bf4-4526-b0f1-8239c05525e7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch, os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from grpo import set_random_seed\n",
    "\n",
    "# Call the function to set random seed for reproducibility\n",
    "set_random_seed(42)\n",
    "\n",
    "# Set environment variables for Weights & Biases (wandb) logging\n",
    "os.environ[\"WANDB_API_KEY\"] = \"YOUR WANDB API KEY\"\n",
    "os.environ[\"WANDB_PROJECT\"] = \"YOUR WANDB PROJECT NAME\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e35173a-b617-4dde-ba8c-d9ad20ea51ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gsm8k_data import gsm8k_metric, extract_gsm8k_answer\n",
    "\n",
    "def evaluate_model(model, tokenizer, eval_examples, device):\n",
    "\n",
    "   model.eval()\n",
    "   correct = 0\n",
    "   total = len(eval_examples)\n",
    "   print(\"\\n\" + \"=\"*50)\n",
    "   print(\"EVALUATION ON\", total, \"EXAMPLES\")\n",
    "   print(\"=\"*50)\n",
    "\n",
    "   for example in eval_examples:\n",
    "       # Get the prompt and expected answer\n",
    "       full_prompt = example[\"prompt\"]\n",
    "       expected = example[\"answer\"]\n",
    "\n",
    "       # Tokenize and generate response\n",
    "       inputs = tokenizer.encode(full_prompt, return_tensors=\"pt\").to(device)\n",
    "       with torch.no_grad():\n",
    "           outputs = model.generate(\n",
    "               inputs,\n",
    "               max_new_tokens=512,\n",
    "               temperature=0.7,\n",
    "               num_return_sequences=1,\n",
    "               pad_token_id=tokenizer.pad_token_id,\n",
    "               eos_token_id=tokenizer.eos_token_id,\n",
    "               forced_eos_token_id=tokenizer.eos_token_id,\n",
    "               early_stopping=False,\n",
    "           ) # 'forward generation --> RL on reward' | can we do the same with pre-training ?\n",
    "       response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "       try:           \n",
    "           # 1. extract functional \n",
    "           predicted = extract_gsm8k_answer(response)\n",
    "           # 2. metric-based reward fn\n",
    "           is_correct = gsm8k_metric(predicted, expected)\n",
    "\n",
    "           # Update counter for correct answers\n",
    "           if is_correct:\n",
    "               correct += 1\n",
    "\n",
    "           # Print evaluation details\n",
    "           print(\"\\nPrompt:\")\n",
    "           print(full_prompt)\n",
    "           print(\"\\nExpected Answer:\")\n",
    "           print(expected)\n",
    "           print(\"\\nExtracted Answer:\")\n",
    "           print(predicted)\n",
    "           print(\"\\nFull Generated Response:\")\n",
    "           print(response)\n",
    "           print(\"\\nCorrect:\", \"✓\" if is_correct else \"✗\")\n",
    "           print(\"-\"*50)\n",
    "\n",
    "       except Exception as e:\n",
    "           print(\"\\nFailed to parse model output for prompt:\")\n",
    "           print(full_prompt)\n",
    "           print(\"Error:\", e)\n",
    "           print(\"-\"*50)\n",
    "\n",
    "   # Calculate and print final accuracy\n",
    "   accuracy = (correct / total) * 100\n",
    "   print(f\"\\nAccuracy: {accuracy:.2f}% ({correct}/{total})\")\n",
    "   print(\"=\"*50)\n",
    "\n",
    "   # Return model to training mode\n",
    "   model.train()\n",
    "   return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d10965-282c-42c6-a005-487c1ba6a8d8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Main execution\n",
    "from gsm8k_data import prepare_dataset, reward_fn\n",
    "from grpo import *\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using primary device: {device}\")\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "output_dir = \"math_solver_model\"\n",
    "\n",
    "print(\"Downloading model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "print(\"Model downloaded\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Detected {num_gpus} GPUs\")\n",
    "device_ids = list(range(num_gpus)) if num_gpus > 1 else None\n",
    "\n",
    "all_data = prepare_dataset(\"train\")\n",
    "random.shuffle(all_data)\n",
    "size_of_eval_data = 30 # change to a smaller value to save time or to a larger number for a more reliable estimate\n",
    "eval_data = all_data[:size_of_eval_data]\n",
    "train_data = all_data[size_of_eval_data:]\n",
    "\n",
    "print(\"\\nInitial model evaluation before finetuning:\")\n",
    "pre_grpo_accuracy = evaluate_model(model, tokenizer, eval_data, device)\n",
    "print(f\"Pre-GRPO Accuracy: {pre_grpo_accuracy:.2f}%\")\n",
    "\n",
    "model = optimize_model_memory(model)\n",
    "\n",
    "print(\"\\nStarting RL fine-tuning using GRPO...\")\n",
    "# This config was tested on a 8xA100 node, where each A100 is has 80GB of VRAM\n",
    "training_config = {\n",
    "    'num_iterations': 1,\n",
    "    'num_steps': 500,\n",
    "    'batch_size': 7, # reduce if you have fewer GPUs\n",
    "    'num_generations': 12, # reduce if you have GPUs with less VRAM\n",
    "    'max_completion_length': 400, # reduce if you have GPUs with less VRAM\n",
    "    'beta': 0.04,\n",
    "    'learning_rate': 5e-6,\n",
    "    'mu': 1,\n",
    "    'epsilon': 0.1\n",
    "}\n",
    "\n",
    "# Initialize Weights & Biases\n",
    "wandb.init(project=os.environ[\"WANDB_PROJECT\"], reinit=True)\n",
    "print(\"Weights & Biases initialized.\")\n",
    "\n",
    "model = train_with_grpo(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_data=train_data,\n",
    "    reward_function=reward_fn,\n",
    "    device_ids=device_ids,\n",
    "    **training_config\n",
    ")\n",
    "\n",
    "wandb.finish()\n",
    "print(\"Training completed and wandb run finished.\")\n",
    "\n",
    "print(\"\\nFinal model evaluation after GRPO RL fine-tuning:\")\n",
    "post_grpo_accuracy = evaluate_model(model, tokenizer, eval_data, device)\n",
    "print(f\"Post-GRPO Accuracy: {post_grpo_accuracy:.2f}%\")\n",
    "\n",
    "print(\"\\nSaving GRPO fine-tuned model...\")\n",
    "model.save_pretrained(\"grpo_finetuned_model\")\n",
    "tokenizer.save_pretrained(\"grpo_finetuned_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
